{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO0Ee2sdM4ai7NI/oYQauuQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PYYDLAfKWt_","executionInfo":{"status":"ok","timestamp":1709936637122,"user_tz":-330,"elapsed":28,"user":{"displayName":"prince francis","userId":"12985198117174015241"}},"outputId":"96932fc0-0a2d-4e0d-b257-8cab2baf7a91"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Mar  8 22:23:56 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","source":["# Check for GPU\n","import torch\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40sIYmHEKjDH","executionInfo":{"status":"ok","timestamp":1709936682261,"user_tz":-330,"elapsed":3489,"user":{"displayName":"prince francis","userId":"12985198117174015241"}},"outputId":"f01901d0-e9f2-40a4-f9d7-edd3f4826565"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["### Creating a device variable to store what kind of device is available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"1e9pLYPYKxNm","executionInfo":{"status":"ok","timestamp":1709936733393,"user_tz":-330,"elapsed":10,"user":{"displayName":"prince francis","userId":"12985198117174015241"}},"outputId":"54a7f443-d850-44b4-fd43-3474ec7462a8"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Count number of devices\n","torch.cuda.device_count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lerQ9PWvK-bs","executionInfo":{"status":"ok","timestamp":1709936764974,"user_tz":-330,"elapsed":421,"user":{"displayName":"prince francis","userId":"12985198117174015241"}},"outputId":"99aa5c79-95d6-431a-b2b8-98303b86ba7e"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Putting tensors (and models) on the GPU\n","# Create tensor (default on CPU)\n","tensor = torch.tensor([1,2,3])\n","\n","## Tensor not on GPU\n","print(tensor,tensor.device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"97FFcQfyLEUH","executionInfo":{"status":"ok","timestamp":1709936886303,"user_tz":-330,"elapsed":6,"user":{"displayName":"prince francis","userId":"12985198117174015241"}},"outputId":"8152e8cc-2df6-4dfc-ba2d-31427f17742c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3]) cpu\n"]}]},{"cell_type":"code","source":["## Move tensor to GPU(if available)\n","tensor_on_gpu = tensor.to(device)\n","tensor_on_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5bOHz68Lj7x","executionInfo":{"status":"ok","timestamp":1709936920091,"user_tz":-330,"elapsed":632,"user":{"displayName":"prince francis","userId":"12985198117174015241"}},"outputId":"21c41577-27ec-4f3d-95a0-f64b5598739d"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["## Moving tensors back to the CPU\n","# If tensor is on GPU, can't transform it to NumPy (this will error)\n","tensor_on_gpu.numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"YzF4OjtxLr5P","executionInfo":{"status":"error","timestamp":1709936953153,"user_tz":-330,"elapsed":8,"user":{"displayName":"prince francis","userId":"12985198117174015241"}},"outputId":"fd29768a-ed05-4a4c-c083-33060f0b1acf"},"execution_count":8,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-39c762170fd1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Moving tensors back to the CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# If tensor is on GPU, can't transform it to NumPy (this will error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtensor_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]}]},{"cell_type":"code","source":["# Instead, copy the tensor back to cpu\n","tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n","tensor_back_on_cpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmWvBmdlL0C-","executionInfo":{"status":"ok","timestamp":1709936994941,"user_tz":-330,"elapsed":6,"user":{"displayName":"prince francis","userId":"12985198117174015241"}},"outputId":"db7fbd54-99b2-460d-887f-cf1f96663096"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 3])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["tensor_on_gpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbvNKeRvL-WB","executionInfo":{"status":"ok","timestamp":1709937022536,"user_tz":-330,"elapsed":10,"user":{"displayName":"prince francis","userId":"12985198117174015241"}},"outputId":"4d7b15df-6131-496d-e4ff-3db43f981ca3"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[],"metadata":{"id":"gMF42psnME-3"},"execution_count":null,"outputs":[]}]}